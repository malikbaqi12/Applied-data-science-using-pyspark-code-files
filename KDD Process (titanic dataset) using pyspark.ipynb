{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malikbaqi12/Applied-data-science-using-pyspark-code-files/blob/main/KDD%20Process%20(titanic%20dataset)%20using%20pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:04.020859Z",
          "iopub.status.busy": "2020-09-01T13:01:04.020260Z",
          "iopub.status.idle": "2020-09-01T13:01:37.056093Z",
          "shell.execute_reply": "2020-09-01T13:01:37.055185Z"
        },
        "papermill": {
          "duration": 33.05125,
          "end_time": "2020-09-01T13:01:37.056315",
          "exception": false,
          "start_time": "2020-09-01T13:01:04.005065",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExukaxphqFC7",
        "outputId": "66a11a94-7ab7-45d8-c742-9adb727c713f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050126,
          "end_time": "2020-09-01T13:01:37.157980",
          "exception": false,
          "start_time": "2020-09-01T13:01:37.107854",
          "status": "completed"
        },
        "tags": [],
        "id": "If9Kd1TZqFDB"
      },
      "source": [
        "First, we need to start a SparkSession and create a spark instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:37.263091Z",
          "iopub.status.busy": "2020-09-01T13:01:37.262491Z",
          "iopub.status.idle": "2020-09-01T13:01:43.252792Z",
          "shell.execute_reply": "2020-09-01T13:01:43.253912Z"
        },
        "papermill": {
          "duration": 6.046023,
          "end_time": "2020-09-01T13:01:43.254118",
          "exception": false,
          "start_time": "2020-09-01T13:01:37.208095",
          "status": "completed"
        },
        "tags": [],
        "id": "I7nLGptiqFDC"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('classification').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:43.380013Z",
          "iopub.status.busy": "2020-09-01T13:01:43.379017Z",
          "iopub.status.idle": "2020-09-01T13:01:43.382883Z",
          "shell.execute_reply": "2020-09-01T13:01:43.381991Z"
        },
        "papermill": {
          "duration": 0.070283,
          "end_time": "2020-09-01T13:01:43.383061",
          "exception": false,
          "start_time": "2020-09-01T13:01:43.312778",
          "status": "completed"
        },
        "tags": [],
        "id": "DqXMnLqdqFDD"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from pyspark.sql.functions import count, mean, when, lit, create_map, regexp_extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.049226,
          "end_time": "2020-09-01T13:01:43.485758",
          "exception": false,
          "start_time": "2020-09-01T13:01:43.436532",
          "status": "completed"
        },
        "tags": [],
        "id": "FEiHCEehqFDE"
      },
      "source": [
        "Reading CSV files in Spark is not that different than in Pandas. The new thing here is the schema of the dataframe. Spark schema is the structure of the DataFrame or Dataset. The columns in the dataframe can be integer or float or string. If we enable inferring schema while reading the dataframe it finds the right schema for each of the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:43.590569Z",
          "iopub.status.busy": "2020-09-01T13:01:43.589929Z",
          "iopub.status.idle": "2020-09-01T13:01:48.794877Z",
          "shell.execute_reply": "2020-09-01T13:01:48.795333Z"
        },
        "papermill": {
          "duration": 5.26008,
          "end_time": "2020-09-01T13:01:48.795476",
          "exception": false,
          "start_time": "2020-09-01T13:01:43.535396",
          "status": "completed"
        },
        "tags": [],
        "id": "Z8cTrcPiqFDF"
      },
      "outputs": [],
      "source": [
        "df1 = spark.read.csv('train.csv',\\\n",
        "                     header=True, inferSchema=True)\n",
        "df2 = spark.read.csv('test.csv', \\\n",
        "                     header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.048466,
          "end_time": "2020-09-01T13:01:48.894106",
          "exception": false,
          "start_time": "2020-09-01T13:01:48.845640",
          "status": "completed"
        },
        "tags": [],
        "id": "NHhiOd1IqFDG"
      },
      "source": [
        "We can view the schema here. There is no need to print the column names separately. Although this option is available in Spark. We will use it shortly for a different purposes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:48.997622Z",
          "iopub.status.busy": "2020-09-01T13:01:48.997051Z",
          "iopub.status.idle": "2020-09-01T13:01:49.012853Z",
          "shell.execute_reply": "2020-09-01T13:01:49.013419Z"
        },
        "papermill": {
          "duration": 0.070609,
          "end_time": "2020-09-01T13:01:49.013584",
          "exception": false,
          "start_time": "2020-09-01T13:01:48.942975",
          "status": "completed"
        },
        "tags": [],
        "id": "4Z3kRSvMqFDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf6f920-2895-428f-c8ab-a9cbbae4f178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.048398,
          "end_time": "2020-09-01T13:01:49.118097",
          "exception": false,
          "start_time": "2020-09-01T13:01:49.069699",
          "status": "completed"
        },
        "tags": [],
        "id": "jaNKs-Q7qFDI"
      },
      "source": [
        "Pandas has head(), tail(), sample() options to view few rows of the dataframe. Spark dataframe also has all of those options which you can try. But here we use show(n) method to view sample rows of the dataframe. By default show() presents 20 rows. Please remember the discussion of transformation vs action above. All of these are example of Spark action. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:49.221251Z",
          "iopub.status.busy": "2020-09-01T13:01:49.220340Z",
          "iopub.status.idle": "2020-09-01T13:01:49.626930Z",
          "shell.execute_reply": "2020-09-01T13:01:49.625913Z"
        },
        "papermill": {
          "duration": 0.459715,
          "end_time": "2020-09-01T13:01:49.627102",
          "exception": false,
          "start_time": "2020-09-01T13:01:49.167387",
          "status": "completed"
        },
        "tags": [],
        "id": "-Rzc8o0EqFDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a691510b-9b1b-4f5d-a3a5-0f2267a464a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050844,
          "end_time": "2020-09-01T13:01:49.738867",
          "exception": false,
          "start_time": "2020-09-01T13:01:49.688023",
          "status": "completed"
        },
        "tags": [],
        "id": "l04ltk_MqFDK"
      },
      "source": [
        "The output of the show() might look ugly, especially if there are a large number of columns in the dataframe. At this point, we might miss Pandas head(). There is an option to convert the Spark dataframe into the Pandas dataframe. But we have to be careful here. Usually, Spark is handling a large volume of data, and converting it to Pandas stores everything immediately to the memory. Which we should avoid for the large data. However, there is a way out. Please remember the lazy evaluation of the Spark transformation. We can transform the Spark dataframe by limit(n) to take only n number of rows and then convert that to the Pandas. toPandas() is an action. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:49.869412Z",
          "iopub.status.busy": "2020-09-01T13:01:49.868444Z",
          "iopub.status.idle": "2020-09-01T13:01:50.100654Z",
          "shell.execute_reply": "2020-09-01T13:01:50.100116Z"
        },
        "papermill": {
          "duration": 0.306945,
          "end_time": "2020-09-01T13:01:50.100788",
          "exception": false,
          "start_time": "2020-09-01T13:01:49.793843",
          "status": "completed"
        },
        "tags": [],
        "id": "2QYwqn88qFDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7332a035-d70c-4692-cd56-9b5d2300b144"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500  None        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250  None        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500  None        S  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7241ef57-0ba8-4ff7-a379-b998fba7dd45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>None</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>None</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>None</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7241ef57-0ba8-4ff7-a379-b998fba7dd45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7241ef57-0ba8-4ff7-a379-b998fba7dd45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7241ef57-0ba8-4ff7-a379-b998fba7dd45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "df1.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.053542,
          "end_time": "2020-09-01T13:01:50.205952",
          "exception": false,
          "start_time": "2020-09-01T13:01:50.152410",
          "status": "completed"
        },
        "tags": [],
        "id": "r-_BxLu-qFDM"
      },
      "source": [
        "Alternatively we can select() a few columns and inspect within Spark. select() is an example of Spark transformation. Therefore that step is evaluated lazily. Hence we pass a Spark action show() at the end to print the result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:50.312996Z",
          "iopub.status.busy": "2020-09-01T13:01:50.311983Z",
          "iopub.status.idle": "2020-09-01T13:01:50.601916Z",
          "shell.execute_reply": "2020-09-01T13:01:50.600913Z"
        },
        "papermill": {
          "duration": 0.346508,
          "end_time": "2020-09-01T13:01:50.602087",
          "exception": false,
          "start_time": "2020-09-01T13:01:50.255579",
          "status": "completed"
        },
        "tags": [],
        "id": "W-Y91TFZqFDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ff6e74-a55d-4646-ce14-3621e5a8fed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+-------+\n",
            "|Survived|Pclass| Age|   Fare|\n",
            "+--------+------+----+-------+\n",
            "|       0|     3|22.0|   7.25|\n",
            "|       1|     1|38.0|71.2833|\n",
            "|       1|     3|26.0|  7.925|\n",
            "|       1|     1|35.0|   53.1|\n",
            "+--------+------+----+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.select('Survived', 'Pclass', 'Age', 'Fare').show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.078811,
          "end_time": "2020-09-01T13:01:50.736893",
          "exception": false,
          "start_time": "2020-09-01T13:01:50.658082",
          "status": "completed"
        },
        "tags": [],
        "id": "KJGE5GvUqFDQ"
      },
      "source": [
        "Spark has describe() method similar to the Pandas. But I find a summary() method more versatile than describe(). Please check [here](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb) for detail. Both describe() and summary() are Spark trasnformations. Therefore, they do not produce result immediately. Hence we need show() at the end. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:50.909545Z",
          "iopub.status.busy": "2020-09-01T13:01:50.907675Z",
          "iopub.status.idle": "2020-09-01T13:01:52.341191Z",
          "shell.execute_reply": "2020-09-01T13:01:52.340149Z"
        },
        "papermill": {
          "duration": 1.524007,
          "end_time": "2020-09-01T13:01:52.341367",
          "exception": false,
          "start_time": "2020-09-01T13:01:50.817360",
          "status": "completed"
        },
        "tags": [],
        "id": "u27XpX9lqFDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eea19c1-b6fe-4693-e560-728f693b5607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "|summary|           Survived|            Pclass|               Age|             Fare|\n",
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "|  count|                891|               891|               714|              891|\n",
            "|   mean| 0.3838383838383838| 2.308641975308642| 29.69911764705882| 32.2042079685746|\n",
            "| stddev|0.48659245426485753|0.8360712409770491|14.526497332334035|49.69342859718089|\n",
            "|    min|                  0|                 1|              0.42|              0.0|\n",
            "|    25%|                  0|                 2|              20.0|           7.8958|\n",
            "|    50%|                  0|                 3|              28.0|          14.4542|\n",
            "|    75%|                  1|                 3|              38.0|             31.0|\n",
            "|    max|                  1|                 3|              80.0|         512.3292|\n",
            "+-------+-------------------+------------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.select('Survived', 'Pclass', 'Age', 'Fare').summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.048925,
          "end_time": "2020-09-01T13:01:52.458132",
          "exception": false,
          "start_time": "2020-09-01T13:01:52.409207",
          "status": "completed"
        },
        "tags": [],
        "id": "iABJuUqfqFDR"
      },
      "source": [
        "count() acts differently in Pandas and Spark. In Spark, it gives the total number of rows in the dataframe. There is no direct way to find the shape of the dataframe. We can use the following trick.  Here count and columns are action. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:52.561809Z",
          "iopub.status.busy": "2020-09-01T13:01:52.560862Z",
          "iopub.status.idle": "2020-09-01T13:01:52.877306Z",
          "shell.execute_reply": "2020-09-01T13:01:52.876833Z"
        },
        "papermill": {
          "duration": 0.369987,
          "end_time": "2020-09-01T13:01:52.877423",
          "exception": false,
          "start_time": "2020-09-01T13:01:52.507436",
          "status": "completed"
        },
        "tags": [],
        "id": "bAvZmzAHqFDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44b1259-7555-4df9-9c44-f4ac3783fd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: \t 891\n",
            "Number of columns: \t 12\n"
          ]
        }
      ],
      "source": [
        "print('Number of rows: \\t', df1.count())\n",
        "print('Number of columns: \\t', len(df1.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051586,
          "end_time": "2020-09-01T13:01:52.988459",
          "exception": false,
          "start_time": "2020-09-01T13:01:52.936873",
          "status": "completed"
        },
        "tags": [],
        "id": "bgN_VcbmqFDT"
      },
      "source": [
        "# Exploratory Data Analysis \n",
        "\n",
        "### About the data visualization in the Spark  \n",
        "\n",
        "There is no native visualization library in Spark. But we can do the lazy transformation on the dataframe, extract the necessary numbers, and make the visualizations out of that. The implementation of this idea can be found [here](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Data_visualization_in_pySpark%20.ipynb). \n",
        "\n",
        "There are options to make visualization by extending other libraries though. However, we do not go to that route here. We will focus on tabular visualization. Tabular visualization is not a bad option. \n",
        "\n",
        "### How many people survived?\n",
        "\n",
        "We can use groupby() and count() transformations to do that. Both are lazy transformation. Again remember that the Spark transformation alone don't evaluate things unless we call an action upon them. Here show() is an action to print the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:53.098306Z",
          "iopub.status.busy": "2020-09-01T13:01:53.096846Z",
          "iopub.status.idle": "2020-09-01T13:01:55.104206Z",
          "shell.execute_reply": "2020-09-01T13:01:55.105105Z"
        },
        "papermill": {
          "duration": 2.065341,
          "end_time": "2020-09-01T13:01:55.105329",
          "exception": false,
          "start_time": "2020-09-01T13:01:53.039988",
          "status": "completed"
        },
        "tags": [],
        "id": "J9r8NE2tqFDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1d6ebf-e4f3-4988-f4fa-948d2149d4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|Survived|count|\n",
            "+--------+-----+\n",
            "|       1|  342|\n",
            "|       0|  549|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.076797,
          "end_time": "2020-09-01T13:01:55.236931",
          "exception": false,
          "start_time": "2020-09-01T13:01:55.160134",
          "status": "completed"
        },
        "tags": [],
        "id": "F3Ub61Y0qFDV"
      },
      "source": [
        "### Continious variables\n",
        "\n",
        "Among the features, Fare and Age are the continuous variables (non-categorical). We can inspect them closely here. Our interest would be to find average fare and age. We already use summary() to calculate mean. If we just want mean we can either to summary('mean') or we can also directly call mean() and select columns inside that. Also in summary we can pass multiple arguments like df.summary('mean', 'stddev') and so on. Again please refer to [this](https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb) link for detail. Here groupby() and mean() are Spark transformation. Now you probably started to figure out which is transformation and which is action. I will stop iterating that in every single case now on. \n",
        "\n",
        "Passenger paying more money for the fair is likely to survive than those paying less. This variable might have collinearity with the passenger class that we investigate later. Age seems to be not that important for survival compared to fare.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:55.390897Z",
          "iopub.status.busy": "2020-09-01T13:01:55.380044Z",
          "iopub.status.idle": "2020-09-01T13:01:56.812593Z",
          "shell.execute_reply": "2020-09-01T13:01:56.811163Z"
        },
        "papermill": {
          "duration": 1.496601,
          "end_time": "2020-09-01T13:01:56.812890",
          "exception": false,
          "start_time": "2020-09-01T13:01:55.316289",
          "status": "completed"
        },
        "tags": [],
        "id": "N8CMZoGHqFDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbd39ad-fa1a-4384-f0fc-db402ac6c421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+------------------+\n",
            "|Survived|         avg(Fare)|          avg(Age)|\n",
            "+--------+------------------+------------------+\n",
            "|       1| 48.39540760233917|28.343689655172415|\n",
            "|       0|22.117886885245877| 30.62617924528302|\n",
            "+--------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').mean('Fare', 'Age').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051579,
          "end_time": "2020-09-01T13:01:56.918525",
          "exception": false,
          "start_time": "2020-09-01T13:01:56.866946",
          "status": "completed"
        },
        "tags": [],
        "id": "D45-d1aGqFDX"
      },
      "source": [
        "### Categorical variables\n",
        "\n",
        "Here we see how each of the categorical variables has affected the survival of the passenger. Spark dataframe also has a pivot() method very similar to the Pandas dataframe to perform this task. \n",
        "\n",
        "Below we see that sex is an (probably the most) important factor for survival. The survival ratio of the female is much higher than that of male. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:01:57.027559Z",
          "iopub.status.busy": "2020-09-01T13:01:57.026124Z",
          "iopub.status.idle": "2020-09-01T13:02:00.318003Z",
          "shell.execute_reply": "2020-09-01T13:02:00.317174Z"
        },
        "papermill": {
          "duration": 3.348212,
          "end_time": "2020-09-01T13:02:00.318170",
          "exception": false,
          "start_time": "2020-09-01T13:01:56.969958",
          "status": "completed"
        },
        "tags": [],
        "id": "39uAqUO7qFDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4efd96-3b7d-454e-b832-fbdbc22fbf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+\n",
            "|Survived|female|male|\n",
            "+--------+------+----+\n",
            "|       1|   233| 109|\n",
            "|       0|    81| 468|\n",
            "+--------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').pivot('Sex').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.071088,
          "end_time": "2020-09-01T13:02:00.458210",
          "exception": false,
          "start_time": "2020-09-01T13:02:00.387122",
          "status": "completed"
        },
        "tags": [],
        "id": "L2suoQCLqFDZ"
      },
      "source": [
        "Similarly, the first-class passengers are more likely to survive than the second class. And the third class passengers had very hard luck. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:00.565692Z",
          "iopub.status.busy": "2020-09-01T13:02:00.564755Z",
          "iopub.status.idle": "2020-09-01T13:02:02.951684Z",
          "shell.execute_reply": "2020-09-01T13:02:02.950314Z"
        },
        "papermill": {
          "duration": 2.442633,
          "end_time": "2020-09-01T13:02:02.951861",
          "exception": false,
          "start_time": "2020-09-01T13:02:00.509228",
          "status": "completed"
        },
        "tags": [],
        "id": "-1PZRFjPqFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d023e896-75c3-4868-b163-b45c2bd85efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+\n",
            "|Survived|  1|  2|  3|\n",
            "+--------+---+---+---+\n",
            "|       1|136| 87|119|\n",
            "|       0| 80| 97|372|\n",
            "+--------+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').pivot('Pclass').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.076122,
          "end_time": "2020-09-01T13:02:03.109036",
          "exception": false,
          "start_time": "2020-09-01T13:02:03.032914",
          "status": "completed"
        },
        "tags": [],
        "id": "fUwG5dOeqFDb"
      },
      "source": [
        "The number of siblings and the number of parents also play some role in their survival. The large family is less likely to survive. Similarly, the person with no companion is also less likely to survive. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:03.219653Z",
          "iopub.status.busy": "2020-09-01T13:02:03.218731Z",
          "iopub.status.idle": "2020-09-01T13:02:05.355317Z",
          "shell.execute_reply": "2020-09-01T13:02:05.354492Z"
        },
        "papermill": {
          "duration": 2.194108,
          "end_time": "2020-09-01T13:02:05.355502",
          "exception": false,
          "start_time": "2020-09-01T13:02:03.161394",
          "status": "completed"
        },
        "tags": [],
        "id": "H3kn37KmqFDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34491128-56f2-4c92-86e6-c0de6ad6fbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+---+---+----+----+\n",
            "|Survived|  0|  1|  2|  3|  4|   5|   8|\n",
            "+--------+---+---+---+---+---+----+----+\n",
            "|       1|210|112| 13|  4|  3|null|null|\n",
            "|       0|398| 97| 15| 12| 15|   5|   7|\n",
            "+--------+---+---+---+---+---+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').pivot('SibSp').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:05.586757Z",
          "iopub.status.busy": "2020-09-01T13:02:05.584771Z",
          "iopub.status.idle": "2020-09-01T13:02:07.517753Z",
          "shell.execute_reply": "2020-09-01T13:02:07.517047Z"
        },
        "papermill": {
          "duration": 2.079707,
          "end_time": "2020-09-01T13:02:07.517920",
          "exception": false,
          "start_time": "2020-09-01T13:02:05.438213",
          "status": "completed"
        },
        "tags": [],
        "id": "H9l71KVGqFDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa7d682-3ea8-48be-cb65-dc4d0fc2f7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+---+----+---+----+\n",
            "|Survived|  0|  1|  2|  3|   4|  5|   6|\n",
            "+--------+---+---+---+---+----+---+----+\n",
            "|       1|233| 65| 40|  3|null|  1|null|\n",
            "|       0|445| 53| 40|  2|   4|  4|   1|\n",
            "+--------+---+---+---+---+----+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').pivot('Parch').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051222,
          "end_time": "2020-09-01T13:02:07.647936",
          "exception": false,
          "start_time": "2020-09-01T13:02:07.596714",
          "status": "completed"
        },
        "tags": [],
        "id": "BwPj4X9KqFDc"
      },
      "source": [
        "Embark also seems to be important. But it can be collinear with Pclass and fair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:07.752084Z",
          "iopub.status.busy": "2020-09-01T13:02:07.750884Z",
          "iopub.status.idle": "2020-09-01T13:02:09.803182Z",
          "shell.execute_reply": "2020-09-01T13:02:09.802372Z"
        },
        "papermill": {
          "duration": 2.105877,
          "end_time": "2020-09-01T13:02:09.803326",
          "exception": false,
          "start_time": "2020-09-01T13:02:07.697449",
          "status": "completed"
        },
        "tags": [],
        "id": "5UL1_9ImqFDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280932d1-8c6c-4276-bcf7-ea39d73f502f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+---+---+---+\n",
            "|Survived|null|  C|  Q|  S|\n",
            "+--------+----+---+---+---+\n",
            "|       1|   2| 93| 30|217|\n",
            "|       0|null| 75| 47|427|\n",
            "+--------+----+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.groupBy('Survived').pivot('Embarked').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051246,
          "end_time": "2020-09-01T13:02:09.910630",
          "exception": false,
          "start_time": "2020-09-01T13:02:09.859384",
          "status": "completed"
        },
        "tags": [],
        "id": "fD-f602uqFDd"
      },
      "source": [
        "# Feature Engineering \n",
        "\n",
        "First, let's see if there are any missed data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:10.015788Z",
          "iopub.status.busy": "2020-09-01T13:02:10.015165Z",
          "iopub.status.idle": "2020-09-01T13:02:11.512651Z",
          "shell.execute_reply": "2020-09-01T13:02:11.511735Z"
        },
        "papermill": {
          "duration": 1.552573,
          "end_time": "2020-09-01T13:02:11.512782",
          "exception": false,
          "start_time": "2020-09-01T13:02:09.960209",
          "status": "completed"
        },
        "tags": [],
        "id": "msqN6Ds0qFDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7228362-a2c8-4ed9-911b-02f79fb03bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId          0\n",
            "Survived             0\n",
            "Pclass               0\n",
            "Name                 0\n",
            "Sex                  0\n",
            "Age                  177\n",
            "SibSp                0\n",
            "Parch                0\n",
            "Ticket               0\n",
            "Fare                 0\n",
            "Cabin                687\n",
            "Embarked             2\n"
          ]
        }
      ],
      "source": [
        "for col in df1.columns:\n",
        "    print(col.ljust(20), df1.filter(df1[col].isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.094838,
          "end_time": "2020-09-01T13:02:11.657639",
          "exception": false,
          "start_time": "2020-09-01T13:02:11.562801",
          "status": "completed"
        },
        "tags": [],
        "id": "-VGVQWcGqFDe"
      },
      "source": [
        "There are many Cabin info is missing. The Cabin is related to Pclass. We will drop this feature. So no problem so far. There are, 2 entries of Embarked missing. We will fill it with the most repeated value S. Age of many people is missing. Again the simplest way to impute the age would be to fill by the average. We choose median for fare imputation. We use Spark's fillna() method to do that. For age we use more complex imputation method discussed below. For now I am just focusing on the train data. There can be different feature missing in the test data. Acutally there is missed fair in test data. So we calculate median fair also. We come to the test data at the end of this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:11.762749Z",
          "iopub.status.busy": "2020-09-01T13:02:11.761755Z",
          "iopub.status.idle": "2020-09-01T13:02:12.038131Z",
          "shell.execute_reply": "2020-09-01T13:02:12.036064Z"
        },
        "papermill": {
          "duration": 0.33109,
          "end_time": "2020-09-01T13:02:12.038320",
          "exception": false,
          "start_time": "2020-09-01T13:02:11.707230",
          "status": "completed"
        },
        "tags": [],
        "id": "BfvHEE5IqFDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fc926b-13cf-4826-92d8-f2559faf9854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+--------+\n",
            "|summary|            Fare|Embarked|\n",
            "+-------+----------------+--------+\n",
            "|   mean|32.2042079685746|    null|\n",
            "|    50%|         14.4542|    null|\n",
            "|    max|        512.3292|       S|\n",
            "+-------+----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.select('Fare', 'Embarked').summary('mean', '50%', 'max').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:12.196295Z",
          "iopub.status.busy": "2020-09-01T13:02:12.195400Z",
          "iopub.status.idle": "2020-09-01T13:02:12.243560Z",
          "shell.execute_reply": "2020-09-01T13:02:12.242488Z"
        },
        "papermill": {
          "duration": 0.136472,
          "end_time": "2020-09-01T13:02:12.243747",
          "exception": false,
          "start_time": "2020-09-01T13:02:12.107275",
          "status": "completed"
        },
        "tags": [],
        "id": "FE0C8IGiqFDf"
      },
      "outputs": [],
      "source": [
        "df1 = df1.fillna({'Embarked': 'S', 'Fare':14.45})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.049361,
          "end_time": "2020-09-01T13:02:12.370700",
          "exception": false,
          "start_time": "2020-09-01T13:02:12.321339",
          "status": "completed"
        },
        "tags": [],
        "id": "ymoNEaXTqFDg"
      },
      "source": [
        "The basic idea for age imputation is to take the title of the people from the name column and impute with the average age of the group of people with that title. Mrs tend to be older than Miss. This method originally appeared in [this](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83) kernel. We will present the pySpark version of the implementation. \n",
        "\n",
        "First, we extract the title using the regular expression and observe the count and average age with each of the titles. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:12.475853Z",
          "iopub.status.busy": "2020-09-01T13:02:12.474988Z",
          "iopub.status.idle": "2020-09-01T13:02:13.640580Z",
          "shell.execute_reply": "2020-09-01T13:02:13.639596Z"
        },
        "papermill": {
          "duration": 1.220382,
          "end_time": "2020-09-01T13:02:13.640844",
          "exception": false,
          "start_time": "2020-09-01T13:02:12.420462",
          "status": "completed"
        },
        "tags": [],
        "id": "So3OXKK2qFDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3401906-1fcd-4b93-8322-27beffe806c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------------+\n",
            "|   Title|count(Age)|          avg(Age)|\n",
            "+--------+----------+------------------+\n",
            "|     Don|         1|              40.0|\n",
            "|Countess|         1|              33.0|\n",
            "|    Lady|         1|              48.0|\n",
            "|     Mme|         1|              24.0|\n",
            "|    Capt|         1|              70.0|\n",
            "|     Sir|         1|              49.0|\n",
            "|Jonkheer|         1|              38.0|\n",
            "|      Ms|         1|              28.0|\n",
            "|     Col|         2|              58.0|\n",
            "|    Mlle|         2|              24.0|\n",
            "|   Major|         2|              48.5|\n",
            "|     Rev|         6|43.166666666666664|\n",
            "|      Dr|         6|              42.0|\n",
            "|  Master|        36| 4.574166666666667|\n",
            "|     Mrs|       108|35.898148148148145|\n",
            "|    Miss|       146|21.773972602739725|\n",
            "|      Mr|       398|32.368090452261306|\n",
            "+--------+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = df1.withColumn('Title', regexp_extract(df1['Name'],\\\n",
        "                '([A-Za-z]+)\\.', 1))\n",
        "\n",
        "df1.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.074338,
          "end_time": "2020-09-01T13:02:13.812909",
          "exception": false,
          "start_time": "2020-09-01T13:02:13.738571",
          "status": "completed"
        },
        "tags": [],
        "id": "SYohZCpJqFDz"
      },
      "source": [
        "It is seen that Mr, Miss, and Mrs are highly repeated than other titles. The count of Master is not that high but its average age is much lower than others. So we keep those four titles and map other with one of the first three. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:13.923557Z",
          "iopub.status.busy": "2020-09-01T13:02:13.922939Z",
          "iopub.status.idle": "2020-09-01T13:02:15.078830Z",
          "shell.execute_reply": "2020-09-01T13:02:15.077818Z"
        },
        "papermill": {
          "duration": 1.215086,
          "end_time": "2020-09-01T13:02:15.079005",
          "exception": false,
          "start_time": "2020-09-01T13:02:13.863919",
          "status": "completed"
        },
        "tags": [],
        "id": "PdntEMoRqFD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c943d2-78bd-4cec-fb6f-1ff8c073cffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "| Title|          avg(Age)|\n",
            "+------+------------------+\n",
            "|  Miss|             21.86|\n",
            "|Master| 4.574166666666667|\n",
            "|    Mr| 33.02272727272727|\n",
            "|   Mrs|35.981818181818184|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "title_dic = {'Mr':'Mr', 'Miss':'Miss', 'Mrs':'Mrs', 'Master':'Master', \\\n",
        "             'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\\\n",
        "             'Don': 'Mr', 'Mme': 'Miss', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\\\n",
        "             'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs', \\\n",
        "             'Dr':'Mr', 'Rev':'Mr'}\n",
        "\n",
        "mapping = create_map([lit(x) for x in chain(*title_dic.items())])\n",
        "\n",
        "df1 = df1.withColumn('Title', mapping[df1['Title']])\n",
        "df1.groupBy('Title').mean('Age').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.063104,
          "end_time": "2020-09-01T13:02:15.196626",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.133522",
          "status": "completed"
        },
        "tags": [],
        "id": "DZANrv0DqFD0"
      },
      "source": [
        "Now we create a function that imputes the age column with the average age of the group of people having the same name title as theirs. And use it to impute the ages in the next stage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:15.312098Z",
          "iopub.status.busy": "2020-09-01T13:02:15.311469Z",
          "iopub.status.idle": "2020-09-01T13:02:15.313793Z",
          "shell.execute_reply": "2020-09-01T13:02:15.314332Z"
        },
        "papermill": {
          "duration": 0.064062,
          "end_time": "2020-09-01T13:02:15.314466",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.250404",
          "status": "completed"
        },
        "tags": [],
        "id": "q5s44KRKqFD1"
      },
      "outputs": [],
      "source": [
        "def age_imputer(df, title, age):\n",
        "    \n",
        "    '''This function search for the null in 'Age' column \n",
        "    of the dataframe df. If there is null then it look \n",
        "    for the title and fill the 'Age' with age argument. \n",
        "    If 'Age' is not null, it will keep the same age.  '''\n",
        "    \n",
        "    return df.withColumn('Age', \\\n",
        "                         when((df['Age'].isNull()) & (df['Title']==title), \\\n",
        "                              age).otherwise(df['Age']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:15.418693Z",
          "iopub.status.busy": "2020-09-01T13:02:15.418058Z",
          "iopub.status.idle": "2020-09-01T13:02:15.506693Z",
          "shell.execute_reply": "2020-09-01T13:02:15.507207Z"
        },
        "papermill": {
          "duration": 0.143294,
          "end_time": "2020-09-01T13:02:15.507384",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.364090",
          "status": "completed"
        },
        "tags": [],
        "id": "Gi6HHE5NqFD1"
      },
      "outputs": [],
      "source": [
        "df1 = age_imputer(df1, 'Mr', 33.02)\n",
        "df1 = age_imputer(df1, 'Mrs', 35.98)\n",
        "df1 = age_imputer(df1, 'Miss', 21.86)\n",
        "df1 = age_imputer(df1, 'Master', 4.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050102,
          "end_time": "2020-09-01T13:02:15.632416",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.582314",
          "status": "completed"
        },
        "tags": [],
        "id": "6Eb8eKLAqFD2"
      },
      "source": [
        "### Creating a new column and dropping a column \n",
        "\n",
        "Now we create a new column called FamilySize combining Parch and SibSp. This API is significantly different in Spark than in Pandas. We use withColumn() method to do that. The first input in the method is a string of the name of the new column. This creates a new column and also keeps the old columns. We will drop the Parch and SibSp column afterward. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:15.741921Z",
          "iopub.status.busy": "2020-09-01T13:02:15.740875Z",
          "iopub.status.idle": "2020-09-01T13:02:15.763010Z",
          "shell.execute_reply": "2020-09-01T13:02:15.762367Z"
        },
        "papermill": {
          "duration": 0.08014,
          "end_time": "2020-09-01T13:02:15.763132",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.682992",
          "status": "completed"
        },
        "tags": [],
        "id": "qBq4HCqzqFD3"
      },
      "outputs": [],
      "source": [
        "df1 = df1.withColumn('FamilySize', df1['Parch'] + df1['SibSp']).\\\n",
        "            drop('Parch', 'SibSp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050996,
          "end_time": "2020-09-01T13:02:15.888878",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.837882",
          "status": "completed"
        },
        "tags": [],
        "id": "C3prflsxqFD3"
      },
      "source": [
        "And drop the unwanted columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:15.996469Z",
          "iopub.status.busy": "2020-09-01T13:02:15.995872Z",
          "iopub.status.idle": "2020-09-01T13:02:16.005801Z",
          "shell.execute_reply": "2020-09-01T13:02:16.006285Z"
        },
        "papermill": {
          "duration": 0.066581,
          "end_time": "2020-09-01T13:02:16.006426",
          "exception": false,
          "start_time": "2020-09-01T13:02:15.939845",
          "status": "completed"
        },
        "tags": [],
        "id": "uWIXjt4jqFD3"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop('PassengerID', 'Cabin', 'Name', 'Ticket', 'Title')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051414,
          "end_time": "2020-09-01T13:02:16.110266",
          "exception": false,
          "start_time": "2020-09-01T13:02:16.058852",
          "status": "completed"
        },
        "tags": [],
        "id": "sI1uApmOqFD4"
      },
      "source": [
        "Now we have a trimmed dataframe. For the small size dataframe show(n) method is not that worse than Pandas head(). See that Sex and Embarked columns are strings. We have to convert them to numeric categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:16.220142Z",
          "iopub.status.busy": "2020-09-01T13:02:16.219223Z",
          "iopub.status.idle": "2020-09-01T13:02:16.518687Z",
          "shell.execute_reply": "2020-09-01T13:02:16.519547Z"
        },
        "papermill": {
          "duration": 0.357147,
          "end_time": "2020-09-01T13:02:16.519785",
          "exception": false,
          "start_time": "2020-09-01T13:02:16.162638",
          "status": "completed"
        },
        "tags": [],
        "id": "mO6LPMIyqFD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5c4722-ac3c-416d-8703-7bfc095d0f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+----------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|FamilySize|\n",
            "+--------+------+------+----+-------+--------+----------+\n",
            "|       0|     3|  male|22.0|   7.25|       S|         1|\n",
            "|       1|     1|female|38.0|71.2833|       C|         1|\n",
            "|       1|     3|female|26.0|  7.925|       S|         0|\n",
            "|       1|     1|female|35.0|   53.1|       S|         1|\n",
            "+--------+------+------+----+-------+--------+----------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.264741,
          "end_time": "2020-09-01T13:02:16.837714",
          "exception": false,
          "start_time": "2020-09-01T13:02:16.572973",
          "status": "completed"
        },
        "tags": [],
        "id": "8yxg_DMNqFD5"
      },
      "source": [
        "And there is no missing value now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:17.007430Z",
          "iopub.status.busy": "2020-09-01T13:02:17.006444Z",
          "iopub.status.idle": "2020-09-01T13:02:17.995949Z",
          "shell.execute_reply": "2020-09-01T13:02:17.994995Z"
        },
        "papermill": {
          "duration": 1.077112,
          "end_time": "2020-09-01T13:02:17.996119",
          "exception": false,
          "start_time": "2020-09-01T13:02:16.919007",
          "status": "completed"
        },
        "tags": [],
        "id": "e2C0cXKLqFD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50d2c11-1da4-49cc-bb05-0cdd4f6c0f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Survived             0\n",
            "Pclass               0\n",
            "Sex                  0\n",
            "Age                  0\n",
            "Fare                 0\n",
            "Embarked             0\n",
            "FamilySize           0\n"
          ]
        }
      ],
      "source": [
        "for col in df1.columns:\n",
        "    print(col.ljust(20), df1.filter(df1[col].isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.079209,
          "end_time": "2020-09-01T13:02:18.169384",
          "exception": false,
          "start_time": "2020-09-01T13:02:18.090175",
          "status": "completed"
        },
        "tags": [],
        "id": "_mmj9eyZqFD6"
      },
      "source": [
        "# Model building \n",
        "\n",
        "So far we used Spark dataframe available in Spark SQL for EDA and feature engineering. Now we will use the Spark ML library to do ML tasks. We will cover the following ML task on Spark ML here:\n",
        "\n",
        "- StringIndexer: Converts string categories to numerical categories. \n",
        "- Vector Assembler: Special to Spark API. We will find detail shortly. \n",
        "- Logistic regression based on Ridge and Lasso regularization. \n",
        "- Tree-based ensemble methods: Random forest and Gradient boosting. \n",
        "- Pipeline: It is a big deal for big data. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:18.290851Z",
          "iopub.status.busy": "2020-09-01T13:02:18.290256Z",
          "iopub.status.idle": "2020-09-01T13:02:18.486744Z",
          "shell.execute_reply": "2020-09-01T13:02:18.486094Z"
        },
        "papermill": {
          "duration": 0.264916,
          "end_time": "2020-09-01T13:02:18.486904",
          "exception": false,
          "start_time": "2020-09-01T13:02:18.221988",
          "status": "completed"
        },
        "tags": [],
        "id": "e7oWZykzqFD7"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression,\\\n",
        "                    RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.05013,
          "end_time": "2020-09-01T13:02:18.588688",
          "exception": false,
          "start_time": "2020-09-01T13:02:18.538558",
          "status": "completed"
        },
        "tags": [],
        "id": "-hebK9-rqFD7"
      },
      "source": [
        "### String Indexer \n",
        "\n",
        "We will convert the Sex and Embarked column from string to numeric index. This creates a new column for numeric leaving the original intact. So we will remove them afterward. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:18.702757Z",
          "iopub.status.busy": "2020-09-01T13:02:18.702116Z",
          "iopub.status.idle": "2020-09-01T13:02:19.724649Z",
          "shell.execute_reply": "2020-09-01T13:02:19.723777Z"
        },
        "papermill": {
          "duration": 1.084203,
          "end_time": "2020-09-01T13:02:19.724832",
          "exception": false,
          "start_time": "2020-09-01T13:02:18.640629",
          "status": "completed"
        },
        "tags": [],
        "id": "Kpo2TolCqFD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2f178a-791f-4cc3-aeb8-cbb5635f83d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+-------+----------+------+------+\n",
            "|Survived|Pclass| Age|   Fare|FamilySize|SexNum|EmbNum|\n",
            "+--------+------+----+-------+----------+------+------+\n",
            "|       0|     3|22.0|   7.25|         1|   0.0|   0.0|\n",
            "|       1|     1|38.0|71.2833|         1|   1.0|   1.0|\n",
            "|       1|     3|26.0|  7.925|         0|   1.0|   0.0|\n",
            "|       1|     1|35.0|   53.1|         1|   1.0|   0.0|\n",
            "+--------+------+----+-------+----------+------+------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stringIndex = StringIndexer(inputCols=['Sex', 'Embarked'], \n",
        "                       outputCols=['SexNum', 'EmbNum'])\n",
        "\n",
        "stringIndex_model = stringIndex.fit(df1)\n",
        "\n",
        "df1_ = stringIndex_model.transform(df1).drop('Sex', 'Embarked')\n",
        "df1_.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050351,
          "end_time": "2020-09-01T13:02:19.830623",
          "exception": false,
          "start_time": "2020-09-01T13:02:19.780272",
          "status": "completed"
        },
        "tags": [],
        "id": "1QSaIA89qFD8"
      },
      "source": [
        "### What is VectorAssembler?\n",
        "\n",
        "In Python's scikit learn API the model takes X and y variable in the separation matrix. The target y is usually a column vector and feature X is a matrix. scikit learn accepts X as a matrix of dataframe directly. But Spark API is different here. First, it requires X and y in a single matrix instead of two for the training data. It accepts X only in the prediction part as it should. And also X should be a vector in each row (see the output below) of the dataframe. In short, we can not directly feed the dataframe in the model. We should do what VectorAssembler does. \n",
        "\n",
        "In below, inputCols are the feature columns that are doing to be merged to make a vector in each row and outputCol is the name of the merged column. This is the column that Spark ML identifies as the feature column. It is a common practice to rename this as features as Spark ML identifies this name. If its name is different you have to mention column name when fitting model. Then we can select only the feature column and y column. See the illustration below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:19.941992Z",
          "iopub.status.busy": "2020-09-01T13:02:19.940722Z",
          "iopub.status.idle": "2020-09-01T13:02:20.358842Z",
          "shell.execute_reply": "2020-09-01T13:02:20.359260Z"
        },
        "papermill": {
          "duration": 0.476938,
          "end_time": "2020-09-01T13:02:20.359405",
          "exception": false,
          "start_time": "2020-09-01T13:02:19.882467",
          "status": "completed"
        },
        "tags": [],
        "id": "3xd62I0DqFD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c4e329-37fe-4739-8fe5-858aa1f82c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+--------+\n",
            "|features                      |Survived|\n",
            "+------------------------------+--------+\n",
            "|[3.0,22.0,7.25,1.0,0.0,0.0]   |0       |\n",
            "|[1.0,38.0,71.2833,1.0,1.0,1.0]|1       |\n",
            "|[3.0,26.0,7.925,0.0,1.0,0.0]  |1       |\n",
            "|[1.0,35.0,53.1,1.0,1.0,0.0]   |1       |\n",
            "+------------------------------+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec_asmbl = VectorAssembler(inputCols=df1_.columns[1:], \n",
        "                           outputCol='features')\n",
        "\n",
        "df1_ = vec_asmbl.transform(df1_).select('features', 'Survived')\n",
        "df1_.show(4, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.05028,
          "end_time": "2020-09-01T13:02:20.461827",
          "exception": false,
          "start_time": "2020-09-01T13:02:20.411547",
          "status": "completed"
        },
        "tags": [],
        "id": "zet4QehoqFD9"
      },
      "source": [
        "Now we split the training data into the train and validation part. We split the data into a 7:3 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:20.572760Z",
          "iopub.status.busy": "2020-09-01T13:02:20.571716Z",
          "iopub.status.idle": "2020-09-01T13:02:20.591967Z",
          "shell.execute_reply": "2020-09-01T13:02:20.592471Z"
        },
        "papermill": {
          "duration": 0.0776,
          "end_time": "2020-09-01T13:02:20.592658",
          "exception": false,
          "start_time": "2020-09-01T13:02:20.515058",
          "status": "completed"
        },
        "tags": [],
        "id": "kGwVjDf3qFD9"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df = df1_.randomSplit([0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:20.704426Z",
          "iopub.status.busy": "2020-09-01T13:02:20.703468Z",
          "iopub.status.idle": "2020-09-01T13:02:21.130785Z",
          "shell.execute_reply": "2020-09-01T13:02:21.130215Z"
        },
        "papermill": {
          "duration": 0.482732,
          "end_time": "2020-09-01T13:02:21.130921",
          "exception": false,
          "start_time": "2020-09-01T13:02:20.648189",
          "status": "completed"
        },
        "tags": [],
        "id": "1-arEBCnqFD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb83fa8-732f-4a33-c953-cdd3dfc7cace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+--------+\n",
            "|features             |Survived|\n",
            "+---------------------+--------+\n",
            "|(6,[0,1],[1.0,33.02])|0       |\n",
            "|(6,[0,1],[1.0,33.02])|0       |\n",
            "|(6,[0,1],[1.0,38.0]) |0       |\n",
            "|(6,[0,1],[1.0,39.0]) |0       |\n",
            "+---------------------+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.show(4, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051163,
          "end_time": "2020-09-01T13:02:21.249049",
          "exception": false,
          "start_time": "2020-09-01T13:02:21.197886",
          "status": "completed"
        },
        "tags": [],
        "id": "Unpo2iFSqFD-"
      },
      "source": [
        "In this output form [0] in the bracket can be confusing. 5, [0] means five consecutive entries are zero. Split has not split the data column-wise.\n",
        "\n",
        "\n",
        "### Linear model \n",
        "\n",
        "We study logistic regression here. Spark ML offers elastic net regularization by default. The regularization function is given by \n",
        "\n",
        "$$ \\alpha (\\lambda | {\\bf{w}} |_1) + (1 - \\alpha) \\left(\\frac\\lambda2 |{\\bf{w}}|_2^2 \\right) $$\n",
        "\n",
        "In spark API $\\alpha$ is eleasticNetParam and $\\lambda$ is regParam. We can make our model Ridge by choosing $\\alpha=0$ and Lasso by choosing $\\alpha=1$. \n",
        "\n",
        "Please note that we need to specify the label column at this stage. It the feature column was named differently we had to specify that too here. In Spark, we fit() the model similar to scikit learn but unlike scikit learn we need to name the fitted instance (see comment below). Then we have unified function evaluate() and we call evaluation parameters like predict, accuracy on the evaluation instance. \n",
        "\n",
        "\n",
        "\n",
        "### Evaluation and metric \n",
        "\n",
        "\n",
        "First, we instantiate MulticlassClassificationEvaluator(). We need to specify the metric we want to evaluate at this stage, like metricName='accuracy' in our case. Fitting and evaluating models follow similarly from there. There is an alternative way to evaluate the accuracy scores in the linear model with the following set of commands: \n",
        "\n",
        "- model_name = model.fit(data) \n",
        "- pred = model_name.evaluate(data)\n",
        "- pred.accuracy\n",
        "\n",
        "In this method, there is no need to import MulticlassClassificationEvaluator(). But we stick with the first convention as it provides uniform API for all the models. \n",
        "\n",
        "\n",
        "At this point, I will remind the discussion of transformation and action again. All the machine learning models and preprocessing modules in Spark are transformations. They are evaluated lazily. When we ask for prediction of a model or score of the model then these are Spark action. For example MulticlassClassificationEvaluator() is a transformation while evaluate() is an action. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:21.356966Z",
          "iopub.status.busy": "2020-09-01T13:02:21.356359Z",
          "iopub.status.idle": "2020-09-01T13:02:21.378859Z",
          "shell.execute_reply": "2020-09-01T13:02:21.378047Z"
        },
        "papermill": {
          "duration": 0.079198,
          "end_time": "2020-09-01T13:02:21.379037",
          "exception": false,
          "start_time": "2020-09-01T13:02:21.299839",
          "status": "completed"
        },
        "tags": [],
        "id": "lj-EBIB5qFD_"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', \n",
        "                                          metricName='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:21.494962Z",
          "iopub.status.busy": "2020-09-01T13:02:21.494348Z",
          "iopub.status.idle": "2020-09-01T13:02:24.766268Z",
          "shell.execute_reply": "2020-09-01T13:02:24.765386Z"
        },
        "papermill": {
          "duration": 3.329435,
          "end_time": "2020-09-01T13:02:24.766445",
          "exception": false,
          "start_time": "2020-09-01T13:02:21.437010",
          "status": "completed"
        },
        "tags": [],
        "id": "DExNZNSRqFD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d533b8-af69-4904-8d89-9ee04fd7b493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7720588235294118"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "ridge = LogisticRegression(labelCol='Survived', \n",
        "                        maxIter=100, \n",
        "                        elasticNetParam=0, # Ridge regression is choosen \n",
        "                        regParam=0.03)\n",
        "\n",
        "model = ridge.fit(train_df)\n",
        "pred = model.transform(valid_df)\n",
        "evaluator.evaluate(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:24.915421Z",
          "iopub.status.busy": "2020-09-01T13:02:24.914524Z",
          "iopub.status.idle": "2020-09-01T13:02:32.116872Z",
          "shell.execute_reply": "2020-09-01T13:02:32.115876Z"
        },
        "papermill": {
          "duration": 7.265087,
          "end_time": "2020-09-01T13:02:32.117057",
          "exception": false,
          "start_time": "2020-09-01T13:02:24.851970",
          "status": "completed"
        },
        "tags": [],
        "id": "i8N2q3AXqFEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943466f1-b46b-422c-cb55-f842368ccbf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7683823529411765"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "lasso = LogisticRegression(labelCol='Survived', \n",
        "                           maxIter=100,\n",
        "                           elasticNetParam=1, # Lasso\n",
        "                           regParam=0.0003)\n",
        "\n",
        "model = lasso.fit(train_df)\n",
        "pred = model.transform(valid_df)\n",
        "evaluator.evaluate(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050663,
          "end_time": "2020-09-01T13:02:32.241561",
          "exception": false,
          "start_time": "2020-09-01T13:02:32.190898",
          "status": "completed"
        },
        "tags": [],
        "id": "GxfYMfNsqFEB"
      },
      "source": [
        "Generally, Lasso performs better than Ridge. \n",
        "\n",
        "\n",
        "### Ensemble Tree \n",
        "\n",
        "\n",
        "Currently Spark ML supports two types of ensemble algorithm. Random forest for bagging and gradient boosting for boosting. There is no stacking algorithm available in Spark ML yet. Here we will study both availabel ensemble method both are tree-based methods. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:32.348574Z",
          "iopub.status.busy": "2020-09-01T13:02:32.347810Z",
          "iopub.status.idle": "2020-09-01T13:02:34.674385Z",
          "shell.execute_reply": "2020-09-01T13:02:34.673384Z"
        },
        "papermill": {
          "duration": 2.383069,
          "end_time": "2020-09-01T13:02:34.674567",
          "exception": false,
          "start_time": "2020-09-01T13:02:32.291498",
          "status": "completed"
        },
        "tags": [],
        "id": "7ZhBf0jwqFEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10272bc1-11e7-48f9-9f3b-22ee788d295d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7830882352941176"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "rf = RandomForestClassifier(labelCol='Survived', \n",
        "                           numTrees=100, maxDepth=3)\n",
        "\n",
        "model = rf.fit(train_df)\n",
        "pred = model.transform(valid_df)\n",
        "evaluator.evaluate(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:34.799084Z",
          "iopub.status.busy": "2020-09-01T13:02:34.798218Z",
          "iopub.status.idle": "2020-09-01T13:02:51.546210Z",
          "shell.execute_reply": "2020-09-01T13:02:51.545220Z"
        },
        "papermill": {
          "duration": 16.803686,
          "end_time": "2020-09-01T13:02:51.546406",
          "exception": false,
          "start_time": "2020-09-01T13:02:34.742720",
          "status": "completed"
        },
        "tags": [],
        "id": "OIHUpaG5qFEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81fe500f-32ac-4e9c-bc81-05b5065f9419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8345588235294118"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "gb = GBTClassifier(labelCol='Survived', maxIter=75, maxDepth=3)\n",
        "\n",
        "model = gb.fit(train_df)\n",
        "pred = model.transform(valid_df)\n",
        "evaluator.evaluate(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.052435,
          "end_time": "2020-09-01T13:02:51.668347",
          "exception": false,
          "start_time": "2020-09-01T13:02:51.615912",
          "status": "completed"
        },
        "tags": [],
        "id": "C2ZiHWlnqFED"
      },
      "source": [
        "Generally, the tree-based ensemble method performs better than the linear model and among the tree-based model, gradient boosting performs better than random forest. We may test different method for our final submission.  \n",
        "\n",
        "\n",
        "# Prediction \n",
        "\n",
        "Now we focus on making a prediction on test data and submit the result. We need to follow the exact same procedure for the test data for data cleaning. First we observer the header and see if there are any missing values in the test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:51.775594Z",
          "iopub.status.busy": "2020-09-01T13:02:51.775018Z",
          "iopub.status.idle": "2020-09-01T13:02:51.882466Z",
          "shell.execute_reply": "2020-09-01T13:02:51.881942Z"
        },
        "papermill": {
          "duration": 0.162963,
          "end_time": "2020-09-01T13:02:51.882585",
          "exception": false,
          "start_time": "2020-09-01T13:02:51.719622",
          "status": "completed"
        },
        "tags": [],
        "id": "YrOUPmu0qFED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2356f1a-c20c-4db3-bed5-7f3880db148c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
            "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n",
            "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
            "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|330911|7.8292| null|       Q|\n",
            "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|363272|   7.0| null|       S|\n",
            "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|240276|9.6875| null|       Q|\n",
            "|        895|     3|    Wirz, Mr. Albert|  male|27.0|    0|    0|315154|8.6625| null|       S|\n",
            "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:51.989859Z",
          "iopub.status.busy": "2020-09-01T13:02:51.988907Z",
          "iopub.status.idle": "2020-09-01T13:02:52.879144Z",
          "shell.execute_reply": "2020-09-01T13:02:52.878209Z"
        },
        "papermill": {
          "duration": 0.945876,
          "end_time": "2020-09-01T13:02:52.879469",
          "exception": false,
          "start_time": "2020-09-01T13:02:51.933593",
          "status": "completed"
        },
        "tags": [],
        "id": "uLDnHW9CqFEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78d003d-aa54-4a63-a918-70f32e149a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId          0\n",
            "Pclass               0\n",
            "Name                 0\n",
            "Sex                  0\n",
            "Age                  86\n",
            "SibSp                0\n",
            "Parch                0\n",
            "Ticket               0\n",
            "Fare                 1\n",
            "Cabin                327\n",
            "Embarked             0\n"
          ]
        }
      ],
      "source": [
        "for col in df2.columns:\n",
        "    print(col.ljust(20), df2.filter(df2[col].isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.0525,
          "end_time": "2020-09-01T13:02:52.985375",
          "exception": false,
          "start_time": "2020-09-01T13:02:52.932875",
          "status": "completed"
        },
        "tags": [],
        "id": "nxnBi-AxqFEF"
      },
      "source": [
        "Unlike the train data, there is no missing value in the Embarked column but there is one missing value for fair. And there are few ages missing. Now we fill the missing value by the median fair (of train data, not the test data). We ignore other missing values as we are dropping Cabin from our model. First, we will make a family size feature and drop the unwanted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:53.095828Z",
          "iopub.status.busy": "2020-09-01T13:02:53.093382Z",
          "iopub.status.idle": "2020-09-01T13:02:53.125822Z",
          "shell.execute_reply": "2020-09-01T13:02:53.124382Z"
        },
        "papermill": {
          "duration": 0.088935,
          "end_time": "2020-09-01T13:02:53.126060",
          "exception": false,
          "start_time": "2020-09-01T13:02:53.037125",
          "status": "completed"
        },
        "tags": [],
        "id": "jzCnP1zgqFEF"
      },
      "outputs": [],
      "source": [
        "df2 = df2.fillna({'Embarked': 'S', 'Fare':14.45})\n",
        "df2 = df2.withColumn('FamilySize', df2['Parch'] + df2['SibSp']).\\\n",
        "            drop('Parch', 'SibSp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.084033,
          "end_time": "2020-09-01T13:02:53.269294",
          "exception": false,
          "start_time": "2020-09-01T13:02:53.185261",
          "status": "completed"
        },
        "tags": [],
        "id": "CuRgN_gLqFEG"
      },
      "source": [
        "Now we come to imputing missing age in the test data. We need to follow exactly the same stages as we did in the train data. Only thing we need to be careful is that we are imputing the averages based on the training data but not the test data. You will identify these steps below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:53.388533Z",
          "iopub.status.busy": "2020-09-01T13:02:53.387480Z",
          "iopub.status.idle": "2020-09-01T13:02:54.165986Z",
          "shell.execute_reply": "2020-09-01T13:02:54.165245Z"
        },
        "papermill": {
          "duration": 0.836143,
          "end_time": "2020-09-01T13:02:54.166165",
          "exception": false,
          "start_time": "2020-09-01T13:02:53.330022",
          "status": "completed"
        },
        "tags": [],
        "id": "F4lQjzoaqFEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19f94c8-e921-47a6-a067-cb89cc7952d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+------------------+\n",
            "| Title|count(Age)|          avg(Age)|\n",
            "+------+----------+------------------+\n",
            "|Master|        17| 7.406470588235294|\n",
            "|   Mrs|        63|38.904761904761905|\n",
            "|  Miss|        64|21.774843750000002|\n",
            "|    Mr|       188|32.340425531914896|\n",
            "+------+----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = df2.withColumn('Title', regexp_extract(df2['Name'],\\\n",
        "                '([A-Za-z]+)\\.', 1))\n",
        "\n",
        "df2 = df2.withColumn('Title', mapping[df2['Title']])\n",
        "\n",
        "df2.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:54.276171Z",
          "iopub.status.busy": "2020-09-01T13:02:54.275261Z",
          "iopub.status.idle": "2020-09-01T13:02:54.482624Z",
          "shell.execute_reply": "2020-09-01T13:02:54.481282Z"
        },
        "papermill": {
          "duration": 0.265607,
          "end_time": "2020-09-01T13:02:54.482801",
          "exception": false,
          "start_time": "2020-09-01T13:02:54.217194",
          "status": "completed"
        },
        "tags": [],
        "id": "vlUeg43dqFEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f8882a-f9fa-4041-c9bd-00eea7f6b619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+------+----+------+--------+----------+\n",
            "|PassengerId|Pclass|   Sex| Age|  Fare|Embarked|FamilySize|\n",
            "+-----------+------+------+----+------+--------+----------+\n",
            "|        892|     3|  male|34.5|7.8292|       Q|         0|\n",
            "|        893|     3|female|47.0|   7.0|       S|         1|\n",
            "|        894|     2|  male|62.0|9.6875|       Q|         0|\n",
            "|        895|     3|  male|27.0|8.6625|       S|         0|\n",
            "+-----------+------+------+----+------+--------+----------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = age_imputer(df2, 'Mr', 33.02)\n",
        "df2 = age_imputer(df2, 'Mrs', 35.98)\n",
        "df2 = age_imputer(df2, 'Miss', 21.86)\n",
        "df2 = age_imputer(df2, 'Master', 4.75)\n",
        "\n",
        "df2 = df2.drop('Cabin', 'Name', 'Ticket', 'Title') # keep PassengerId \n",
        "df2.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.04992,
          "end_time": "2020-09-01T13:02:54.584734",
          "exception": false,
          "start_time": "2020-09-01T13:02:54.534814",
          "status": "completed"
        },
        "tags": [],
        "id": "9A7DrozHqFEI"
      },
      "source": [
        "Let's check the missing values again. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:54.692053Z",
          "iopub.status.busy": "2020-09-01T13:02:54.691021Z",
          "iopub.status.idle": "2020-09-01T13:02:55.223467Z",
          "shell.execute_reply": "2020-09-01T13:02:55.223039Z"
        },
        "papermill": {
          "duration": 0.588546,
          "end_time": "2020-09-01T13:02:55.223627",
          "exception": false,
          "start_time": "2020-09-01T13:02:54.635081",
          "status": "completed"
        },
        "tags": [],
        "id": "hRDoyCQZqFEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125a12a7-eec3-4d76-f879-d99ba3f4ea5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId          0\n",
            "Pclass               0\n",
            "Sex                  0\n",
            "Age                  0\n",
            "Fare                 0\n",
            "Embarked             0\n",
            "FamilySize           0\n"
          ]
        }
      ],
      "source": [
        "for col in df2.columns:\n",
        "    print(col.ljust(20), df2.filter(df2[col].isNull()).count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051111,
          "end_time": "2020-09-01T13:02:55.326483",
          "exception": false,
          "start_time": "2020-09-01T13:02:55.275372",
          "status": "completed"
        },
        "tags": [],
        "id": "jbBqbkr5qFEJ"
      },
      "source": [
        "### Pipeline \n",
        "\n",
        "At this stage, it is worth introducing pipeline. In machine learning, it is common to run a sequence of algorithms to process and learn from data. In our example, we performed StringIndexer, VectorAssembler, and ML model. In other cases, the intermediate stages can be standardization, vectorization (for text processing), normalization, etc. These operations have to be performed on a specific order. Spark represents such a workflow as a Pipeline, which consists of a sequence of stages to be run in a specific order. Pipeline chains multiple Transformers and Estimators together to specify an ML workflow. \n",
        "\n",
        "Without the pipeline, we have to execute each stage, store the outcome, and feed into the next stage and evaluate, and so on. We prefer pipeline over this manual approach because of the following reasons: \n",
        "\n",
        "- The pipeline is less prone to mistake because the processes are automated. \n",
        "- In a production environment, this is the only way to do machine learning end to end. \n",
        "- Pipeline enhances the lazy evaluation. So this is a very natural choice in Spark. The pipeline is even more important for big data.\n",
        "\n",
        "\n",
        "### Grid-search and cross-validation \n",
        "\n",
        "Usually, there are many hyperparameters in a model of selection and some combination of those parameters might give the best result. Tuning them requires checking all possible combinations of the hyperparameter. Doing them manually is a tedious bookkeeping task. Fortunately, there is a grid search option available in Spark like in Sci-kit learn. \n",
        "\n",
        "When doing the grid search we need to validate the model using a separate dataset that was not used to train the data. So far we used customized validation set for comparison between different models. Usually, Spark would be handling very big data. For big data, the train-validation split can be sufficient.  For small datasets like this, however, cross-validation is preferred over the train-validation split. Coss-validation is available in Spark. We will use five-fold cross-validation for better model selection. \n",
        "\n",
        "We use CrossValidator available in Spark ML for the cross-validation. CrossValidator accepts estimatorParamMaps in which we can pass a grid search object built with ParamGridBuilder which is also available in Spark ML. \n",
        "\n",
        "We have chosen a random forest for our submission model. We test three hyperparameters from the random forest: number of trees, minimum information gain in each split. The tuning of the number of trees is not that tricky, higher is better. The only concern here is time it takes for a large number of trees taken. The depth of the tree should be tuned properly. Larger depth with some non-zero info gain can give the best performance. Other objects in the model pipeline have no hyperparameters. If they would we could make a grid using those as well. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:02:55.439455Z",
          "iopub.status.busy": "2020-09-01T13:02:55.438520Z",
          "iopub.status.idle": "2020-09-01T13:05:39.476373Z",
          "shell.execute_reply": "2020-09-01T13:05:39.476847Z"
        },
        "papermill": {
          "duration": 164.096641,
          "end_time": "2020-09-01T13:05:39.476999",
          "exception": false,
          "start_time": "2020-09-01T13:02:55.380358",
          "status": "completed"
        },
        "tags": [],
        "id": "f6yFc7__qFEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dc8156-e23f-4c92-d16f-74375c4d093c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8496071829405163"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "pipeline_rf = Pipeline(stages=[stringIndex, vec_asmbl, rf])\n",
        "\n",
        "paramGrid = ParamGridBuilder().\\\n",
        "            addGrid(rf.maxDepth, [3, 4, 5]).\\\n",
        "            addGrid(rf.minInfoGain, [0., 0.01, 0.1]).\\\n",
        "            addGrid(rf.numTrees, [1000]).\\\n",
        "            build()\n",
        "\n",
        "selected_model = CrossValidator(estimator=pipeline_rf, \n",
        "                                estimatorParamMaps=paramGrid, \n",
        "                                evaluator=evaluator, \n",
        "                                numFolds=5)\n",
        "\n",
        "model_final = selected_model.fit(df1)\n",
        "pred_train = model_final.transform(df1)\n",
        "evaluator.evaluate(pred_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.050601,
          "end_time": "2020-09-01T13:05:39.578095",
          "exception": false,
          "start_time": "2020-09-01T13:05:39.527494",
          "status": "completed"
        },
        "tags": [],
        "id": "wgmOmxlbqFEK"
      },
      "source": [
        "This is the in-sample accuracy which is generally higher than the out-sample accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:39.685134Z",
          "iopub.status.busy": "2020-09-01T13:05:39.684521Z",
          "iopub.status.idle": "2020-09-01T13:05:40.351977Z",
          "shell.execute_reply": "2020-09-01T13:05:40.352399Z"
        },
        "papermill": {
          "duration": 0.723499,
          "end_time": "2020-09-01T13:05:40.352544",
          "exception": false,
          "start_time": "2020-09-01T13:05:39.629045",
          "status": "completed"
        },
        "tags": [],
        "id": "TPbYiNT8qFEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1051a387-2ee3-4ee8-9bfb-c7605b3ddea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+\n",
            "|PassengerId|Survived|\n",
            "+-----------+--------+\n",
            "|        892|       0|\n",
            "|        893|       0|\n",
            "|        894|       0|\n",
            "|        895|       0|\n",
            "|        896|       1|\n",
            "+-----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_test = model_final.transform(df2)\n",
        "\n",
        "predictions = pred_test.select('PassengerId', 'prediction')\n",
        "predictions = predictions.\\\n",
        "                withColumn('Survived', predictions['prediction'].\\\n",
        "                cast('integer')).drop('prediction')\n",
        "predictions.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051565,
          "end_time": "2020-09-01T13:05:40.455540",
          "exception": false,
          "start_time": "2020-09-01T13:05:40.403975",
          "status": "completed"
        },
        "tags": [],
        "id": "PZgAVI04qFEM"
      },
      "source": [
        "The following is the method to read csv file in Spark. We can even read the csv file using Spark API. But there is some problem with that, especially Pandas can not read that csv and submission through kernel does not work for it. For this reason we change the submission file to pandas and make a submission.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:40.561263Z",
          "iopub.status.busy": "2020-09-01T13:05:40.560693Z",
          "iopub.status.idle": "2020-09-01T13:05:41.718292Z",
          "shell.execute_reply": "2020-09-01T13:05:41.718776Z"
        },
        "papermill": {
          "duration": 1.211887,
          "end_time": "2020-09-01T13:05:41.718946",
          "exception": false,
          "start_time": "2020-09-01T13:05:40.507059",
          "status": "completed"
        },
        "tags": [],
        "id": "hKPVCVWiqFEM"
      },
      "outputs": [],
      "source": [
        "# Writing csv file in Spark \n",
        "predictions.coalesce(1).write.csv('submission_file.csv', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:41.827284Z",
          "iopub.status.busy": "2020-09-01T13:05:41.826635Z",
          "iopub.status.idle": "2020-09-01T13:05:42.013190Z",
          "shell.execute_reply": "2020-09-01T13:05:42.013931Z"
        },
        "papermill": {
          "duration": 0.242897,
          "end_time": "2020-09-01T13:05:42.014181",
          "exception": false,
          "start_time": "2020-09-01T13:05:41.771284",
          "status": "completed"
        },
        "tags": [],
        "id": "akSHJGjoqFEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f126b3-c557-44ab-fa1c-f5cdcba2a8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+\n",
            "|PassengerId|Survived|\n",
            "+-----------+--------+\n",
            "|        892|       0|\n",
            "|        893|       0|\n",
            "|        894|       0|\n",
            "|        895|       0|\n",
            "+-----------+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Reading the saved file from spark \n",
        "spark.read.csv('submission_file.csv', header=True).show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:42.120056Z",
          "iopub.status.busy": "2020-09-01T13:05:42.119430Z",
          "iopub.status.idle": "2020-09-01T13:05:43.012846Z",
          "shell.execute_reply": "2020-09-01T13:05:43.013325Z"
        },
        "papermill": {
          "duration": 0.948387,
          "end_time": "2020-09-01T13:05:43.013454",
          "exception": false,
          "start_time": "2020-09-01T13:05:42.065067",
          "status": "completed"
        },
        "tags": [],
        "id": "8iNnO6vNqFEO"
      },
      "outputs": [],
      "source": [
        "# Writing csv file using Pandas \n",
        "predictions.toPandas().to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:43.124413Z",
          "iopub.status.busy": "2020-09-01T13:05:43.123517Z",
          "iopub.status.idle": "2020-09-01T13:05:43.133290Z",
          "shell.execute_reply": "2020-09-01T13:05:43.132837Z"
        },
        "papermill": {
          "duration": 0.067385,
          "end_time": "2020-09-01T13:05:43.133391",
          "exception": false,
          "start_time": "2020-09-01T13:05:43.066006",
          "status": "completed"
        },
        "tags": [],
        "id": "H1MEzUx0qFEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c98d5ae7-9df9-4c40-ec61-9ede75ffe6f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived\n",
              "0          892         0\n",
              "1          893         0\n",
              "2          894         0\n",
              "3          895         0\n",
              "4          896         1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b674b3f8-d02b-4e72-a91a-128397d0ab0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b674b3f8-d02b-4e72-a91a-128397d0ab0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b674b3f8-d02b-4e72-a91a-128397d0ab0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b674b3f8-d02b-4e72-a91a-128397d0ab0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Inspecting csv file in pandas \n",
        "import pandas as pd\n",
        "pd.read_csv('submission.csv').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.052088,
          "end_time": "2020-09-01T13:05:43.238809",
          "exception": false,
          "start_time": "2020-09-01T13:05:43.186721",
          "status": "completed"
        },
        "tags": [],
        "id": "lTGxVjanqFEP"
      },
      "source": [
        "We can also save the model itself for future use so that you don't have to train every time.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:43.346189Z",
          "iopub.status.busy": "2020-09-01T13:05:43.345291Z",
          "iopub.status.idle": "2020-09-01T13:05:46.052844Z",
          "shell.execute_reply": "2020-09-01T13:05:46.053789Z"
        },
        "papermill": {
          "duration": 2.763986,
          "end_time": "2020-09-01T13:05:46.054012",
          "exception": false,
          "start_time": "2020-09-01T13:05:43.290026",
          "status": "completed"
        },
        "tags": [],
        "id": "023UQapfqFEP"
      },
      "outputs": [],
      "source": [
        "model_final.write().save('titanic_classification.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-01T13:05:46.176784Z",
          "iopub.status.busy": "2020-09-01T13:05:46.176198Z",
          "iopub.status.idle": "2020-09-01T13:05:46.900761Z",
          "shell.execute_reply": "2020-09-01T13:05:46.900267Z"
        },
        "papermill": {
          "duration": 0.787053,
          "end_time": "2020-09-01T13:05:46.900928",
          "exception": false,
          "start_time": "2020-09-01T13:05:46.113875",
          "status": "completed"
        },
        "tags": [],
        "id": "RdLauWH3qFEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28b98a0-7804-4daa-bc1d-c8803f483593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "titanic_classification.model/bestModel:\n",
            "metadata  stages\n",
            "\n",
            "titanic_classification.model/estimator:\n",
            "metadata  stages\n",
            "\n",
            "titanic_classification.model/evaluator:\n",
            "metadata\n",
            "\n",
            "titanic_classification.model/metadata:\n",
            "part-00000  _SUCCESS\n"
          ]
        }
      ],
      "source": [
        "! ls titanic_classification.model/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.051598,
          "end_time": "2020-09-01T13:05:47.005471",
          "exception": false,
          "start_time": "2020-09-01T13:05:46.953873",
          "status": "completed"
        },
        "tags": [],
        "id": "wI485YSWqFES"
      },
      "source": [
        "# Final thought. \n",
        "\n",
        "- We just presented a base model here and established that Spark is basically capable of doing many tasks on machine learning and model building workflow seamlessly. The model is not fine-tuned yet. It would be interesting to see Spark matching Sci-kit learn's performance. \n",
        "\n",
        "- We did not perform standardization. The reason is standardization is inbuilt in the Spark linear model and it is not needed for the tree-based models. If we had chosen a linear model as our prediction model we may have to turn it off in order to make normalization based on train data instead of test data while making a prediction. \n",
        "\n",
        "- We did not include everything in the pipeline. For example, we imputed null values outside the pipeline. In a production environment, it is required to have everything in the pipeline. So there is room for improvement. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "papermill": {
          "duration": 0.051053,
          "end_time": "2020-09-01T13:05:47.108146",
          "exception": false,
          "start_time": "2020-09-01T13:05:47.057093",
          "status": "completed"
        },
        "tags": [],
        "id": "WF479T0yqFET"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 287.282899,
      "end_time": "2020-09-01T13:05:47.269555",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-09-01T13:00:59.986656",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}